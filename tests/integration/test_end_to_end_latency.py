"""Comprehensive end-to-end test of the real latency measurement system."""

import sys
import json
import time
from pathlib import Path
from datetime import datetime, timezone

# Add src to Python path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

def test_end_to_end_latency():
    """Test the complete latency measurement pipeline from event processing to dashboard data."""
    print("ğŸš€ Starting End-to-End Latency Integration Test")
    print("=" * 60)
    
    try:
        # Phase 1: Import and Initialize Components
        print("\nğŸ“¦ Phase 1: Importing components...")
        from service.main import IntegratedEventHandler, periodic_preprocessing
        from service.runtime_glue import RuntimeGlue, RuntimeConfig
        from data_pipeline.processors.text_rt_distilbert import RealtimeTextEmbedder
        from data_pipeline.storage.builder import GraphBuilder
        from model.inference.spam_filter import SpamScorer
        from model.inference.adaptive_thresholds import SensitivityController
        from config.schemas import Event
        from utils.io import get_hour_bucket, LatencyTimer, LatencyAggregator
        
        print("âœ“ All components imported successfully")
        
        # Phase 2: Initialize Service Components
        print("\nğŸ”§ Phase 2: Initializing service components...")
        spam_scorer = SpamScorer()
        graph_builder = GraphBuilder(spam_scorer=spam_scorer)
        sensitivity_controller = SensitivityController()
        
        # Mock embedder for testing (to avoid heavy model loading)
        class MockEmbedder:
            def __init__(self):
                self.batch_size = 1
                self.max_latency_ms = 10
                self.device = "cpu"
            def encode(self, texts):
                import numpy as np
                # Simulate realistic processing time
                time.sleep(0.002)  # 2ms processing time
                return [np.zeros(768) for _ in texts]
        
        embedder = MockEmbedder()
        
        # Create integrated event handler with latency measurement
        event_handler = IntegratedEventHandler(
            embedder=embedder,
            graph_builder=graph_builder,
            spam_scorer=spam_scorer,
            sensitivity_controller=sensitivity_controller
        )
        
        print("âœ“ IntegratedEventHandler created with LatencyAggregator")
        assert hasattr(event_handler, 'latency_aggregator'), "LatencyAggregator not initialized"
        
        # Phase 3: Test Individual Latency Timer
        print("\nâ±ï¸  Phase 3: Testing individual LatencyTimer...")
        with LatencyTimer() as timer:
            timer.start_stage('test_stage1')
            time.sleep(0.001)
            timer.end_stage('test_stage1')
            
            timer.start_stage('test_stage2')
            time.sleep(0.002)
            timer.end_stage('test_stage2')
        
        print(f"âœ“ Timer recorded total: {timer.total_duration_ms}ms")
        stage_ms = timer.get_stage_ms()
        print(f"âœ“ Stage breakdown: {dict(stage_ms)}")
        assert timer.total_duration_ms > 0, "Timer should record positive duration"
        
        # Phase 4: Test Event Processing with Latency Measurement
        print("\nğŸ¯ Phase 4: Processing events with latency measurement...")
        
        test_events = [
            {
                'event_id': f'perf_test_event_{i}',
                'ts_iso': datetime.now(timezone.utc).isoformat(),
                'actor_id': f'perf_user_{i}',
                'text': f'Performance test content {i} with some realistic length for embedding processing',
                'features': {}
            }
            for i in range(10)
        ]
        
        print(f"Processing {len(test_events)} events...")
        all_scores = []
        
        for i, event in enumerate(test_events):
            scores = event_handler.on_event(event)
            all_scores.append(scores)
            print(f"Event {i+1}: {len(scores)} predictions, sample scores: {list(scores.values())[:3]}")
            time.sleep(0.001)  # Small delay to vary latencies
        
        # Verify latency measurements were collected
        measurements = event_handler.latency_aggregator.measurements
        print(f"\nâœ“ Collected {len(measurements)} latency measurements")
        print(f"  Latency range: {min(measurements)}ms - {max(measurements)}ms")
        
        # Phase 5: Test RuntimeGlue Integration
        print("\nğŸ”— Phase 5: Testing RuntimeGlue integration...")
        
        config = RuntimeConfig(
            update_interval_sec=1,
            enable_background_timer=False,  # Disable for test
            metrics_snapshot_dir="datasets/metrics_hourly"
        )
        
        runtime_glue = RuntimeGlue(event_handler, config)
        
        # Test metrics update (this was causing the original error)
        print("Updating metrics and cache...")
        runtime_glue._update_metrics_and_cache()
        print("âœ“ Metrics update completed successfully")
        
        # Phase 6: Verify Metrics Files
        print("\nğŸ“Š Phase 6: Verifying metrics files...")
        
        metrics_dir = Path("datasets/metrics_hourly")
        if not metrics_dir.exists():
            print("âŒ Metrics directory not found")
            return False
        
        # Find the most recent metrics file
        metrics_files = list(metrics_dir.glob("metrics_*.json"))
        if not metrics_files:
            print("âŒ No metrics files found")
            return False
        
        latest_metrics = max(metrics_files, key=lambda f: f.stat().st_mtime)
        print(f"âœ“ Found latest metrics file: {latest_metrics.name}")
        
        # Verify file contents
        with open(latest_metrics, 'r') as f:
            metrics_data = json.load(f)
        
        latency = metrics_data.get('latency', {})
        if not latency:
            print("âŒ No latency data in metrics file")
            return False
        
        print("âœ“ Latency data structure verified:")
        print(f"  Median: {latency.get('median_ms', 0)}ms")
        print(f"  P95: {latency.get('p95_ms', 0)}ms")
        
        per_stage = latency.get('per_stage_ms', {})
        print(f"  Per-stage breakdown:")
        for stage, ms in per_stage.items():
            print(f"    {stage}: {ms}ms")
        
        # Phase 7: Verify SLO Checking
        print("\nğŸ“‹ Phase 7: Testing SLO compliance...")
        
        slo_status = event_handler.latency_aggregator.meets_slo()
        print(f"âœ“ SLO Status: {slo_status}")
        
        from config.config import SLO_MED_MS, SLO_P95_MS
        print(f"  SLO Thresholds: Median < {SLO_MED_MS}ms, P95 < {SLO_P95_MS}ms")
        print(f"  Current Performance: Median = {latency['median_ms']}ms, P95 = {latency['p95_ms']}ms")
        
        # Phase 8: Success Summary
        print("\nğŸ‰ Phase 8: Test Summary")
        print("=" * 60)
        print("âœ… ALL TESTS PASSED!")
        print("\nReal latency measurement system is fully integrated:")
        print("âœ“ LatencyTimer captures per-stage timing")
        print("âœ“ LatencyAggregator calculates percentiles and SLO compliance")
        print("âœ“ IntegratedEventHandler records real latencies for each event")
        print("âœ“ RuntimeGlue integrates latency data into hourly metrics")
        print("âœ“ Metrics files contain complete latency breakdowns")
        print("âœ“ Dashboard can display real-time latency data")
        print("âœ“ SLO breach detection is functional")
        
        print(f"\nPerformance Summary:")
        print(f"â€¢ Processed {len(test_events)} events")
        if measurements:
            print(f"â€¢ Average latency: {sum(measurements) / len(measurements):.1f}ms")
        else:
            print(f"â€¢ Average latency: No measurements recorded")
        print(f"â€¢ SLO compliance: {'PASS' if all(slo_status.values()) else 'BREACH'}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_end_to_end_latency()
    exit_code = 0 if success else 1
    
    if success:
        print("\n" + "=" * 60)
        print("ğŸŠ LATENCY INTEGRATION COMPLETE!")
        print("The system now captures and displays real latency measurements.")
        print("You can view them in the dashboard at http://localhost:8501")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("ğŸ’¥ INTEGRATION TEST FAILED")
        print("Please check the errors above and fix before proceeding.")
        print("=" * 60)
    
    sys.exit(exit_code)
